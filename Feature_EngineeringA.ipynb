{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is a parameter?"
      ],
      "metadata": {
        "id": "x_EJC1rq4GqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a parameter is a variable that is learned from the data during the training process.It is used to represent the underlying relationships in the data and is used to make predictions on new data.\n",
        "\n",
        "Here are some key points about parameters :\n",
        "\n",
        "1. Learned from data :\n",
        "\n",
        " Parameters are not set manually but are automatically learned by the model during the training process.The model adjusts these parameters to minimize the error between its predictions and the actual values in the training data.\n",
        "\n",
        "2. Represent underlying relationships :\n",
        "\n",
        " Parameters capture the patterns and trends present in the data. For example, in a linear regression model, the parameters represent the slope and intercept of the line that best fits the data points.\n",
        "\n",
        "3. Used for prediction :\n",
        "\n",
        " Once the model has learned the parameters, it can use them to make predictions on new, unseen data.The model applies the learned relationships to generate predictions for these new data points.\n",
        "\n",
        "* Examples of model parameters :\n",
        "\n",
        " * Weights and biases in neural networks :\n",
        "\n",
        "      These parameters determine the strength of the connections between neurons and the overall output of the network.\n",
        "\n",
        " * Coefficients in linear and logistic regression :\n",
        "\n",
        "      These parameters represent the importance of each input feature in determining the output.\n",
        "\n",
        "  * Support vectors in support vector machines :\n",
        "  \n",
        "   These parameters define the decision boundary that separates different classes of data points.\n",
        "\n"
      ],
      "metadata": {
        "id": "CV9ioOja4H5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is correlation? What does negative correlation mean?\n"
      ],
      "metadata": {
        "id": "Y8LiwCLA4IA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation in Machine Learning :\n",
        "\n",
        "Correlation is a statistical measure that indicates the extent to which two variables are related. It quantifies the strength and direction of the relationship between two variables.\n",
        "\n",
        "Types of Correlation:\n",
        "\n",
        "1. Positive Correlation:\n",
        "\n",
        "As one variable increases, the other also increases.\n",
        "\n",
        "Example: Height and Weight. Taller people tend to weigh more.\n",
        "2. Negative Correlation:\n",
        "\n",
        "As one variable increases, the other decreases.\n",
        "\n",
        "Example: Hours of Study and Exam Failures. More study hours typically lead to fewer failures.\n",
        "3. No Correlation:\n",
        "\n",
        "No relationship exists between the two variables.\n",
        "\n",
        "Example: Shoe Size and IQ.\n",
        "\n",
        "* Negative Correlation Explained:\n",
        "\n",
        "A negative correlation indicates an inverse relationship between two variables. When one variable increases, the other tends to decrease. The strength of the negative correlation is measured by the correlation coefficient, which ranges from -1 to 1.\n",
        "\n",
        "  * -1: Perfect negative correlation (strongest inverse relationship)\n",
        "  * 0: No correlation\n",
        "  * 1: Perfect positive correlation (strongest direct relationship)\n",
        "\n"
      ],
      "metadata": {
        "id": "ihgM_ao671sB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "3cfYmGys-AW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning (ML) is a branch of artificial intelligence (AI) that focuses on building systems that can automatically learn from data and improve their performance over time without being explicitly programmed.\n",
        "\n",
        "In essence:\n",
        "\n",
        "* ML algorithms identify patterns in data, build models, and make predictions or decisions based on new input data.\n",
        "* The learning process is driven by the ability of these models to adapt and generalize from examples.\n",
        "\n",
        "Main Components in Machine Learning:\n",
        "\n",
        "1. Data:\n",
        "\n",
        "* Definition:he raw information (examples or observations) that the machine learning model learns from.\n",
        "\n",
        "* Types: 1.Structured (tables with rows and columns).\n",
        "\n",
        "  2.Unstructured (images, audio, text).\n",
        "\n",
        "* Importance: High-quality, diverse, and relevant data is critical for building an effective ML model.\n",
        "\n",
        "2. Features:\n",
        "\n",
        "* Definition: The individual measurable properties or characteristics of the data used by the model.\n",
        "* Feature Engineering: The process of selecting, modifying, or creating new features to improve model performance.\n",
        "\n",
        "3. Model:\n",
        "\n",
        "* Definition: The mathematical representation or algorithm that maps input data (features) to output (predictions).\n",
        "* Examples:\n",
        "\n",
        "  Linear regression.\n",
        "\n",
        "  Neural networks.\n",
        "\n",
        "  Decision trees.\n",
        "\n",
        "4.  Training:\n",
        "\n",
        "* Definition: The process of teaching the model to learn patterns in the data by minimizing a loss function.\n",
        "* Components:\n",
        "  * Training data: Subset of the dataset used for model training.\n",
        "  * Optimization algorithm: Adjusts model parameters (e.g., weights) to reduce the error (e.g., gradient descent).\n",
        "\n",
        "5.  Loss Function:\n",
        "\n",
        "* Definition: A mathematical function that measures the error between the model's predictions and the true values.\n",
        "* Goal: Minimize the loss function to improve the model's performance.\n",
        "* Examples:\n",
        "  * Mean Squared Error (MSE) for regression tasks.\n",
        "  * Cross-Entropy Loss for classification tasks.\n",
        "\n",
        "6.  Testing:\n",
        "\n",
        "* Definition: Using a separate dataset (test data) to evaluate the final model's performance after training and validation.\n",
        "* Ensures that the model generalizes well to unseen data."
      ],
      "metadata": {
        "id": "cYrEP9Fv-JsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "o0PSv2S5aU1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Value: A Key Indicator of Model Performance\n",
        "\n",
        "In machine learning, the loss value is a crucial metric that quantifies how well a model is performing. It essentially measures the discrepancy between the model's predictions and the actual ground truth values.\n",
        "\n",
        "Here's how loss value helps in determining model quality :\n",
        "\n",
        "1. Quantifying Error:\n",
        "\n",
        "Lower Loss, Better Model : A lower loss value indicates that the model's predictions are closer to the true values. Conversely, a higher loss value suggests significant errors in the predictions.\n",
        "\n",
        "Error Minimization : The primary goal of training a machine learning model is to minimize this loss function. By adjusting the model's parameters, the model strives to make predictions that are increasingly accurate.\n",
        "\n",
        "2. Monitoring Training Progress:\n",
        "\n",
        "* Training Curve Analysis : By tracking the loss value during training, you can visualize the model's learning progress.\n",
        "\n",
        "* Identifying Overfitting and Underfitting :\n",
        "\n",
        "  * Overfitting: If the loss on the training set continues to decrease while the loss on a validation set increases, it suggests that the model is overfitting, memorizing the training data rather than learning general patterns.\n",
        "\n",
        "  * Underfitting: If the loss on both the training and validation sets remains high, it indicates that the model is underfitting, failing to capture the underlying patterns in the data.\n",
        "\n",
        "3.  Model Selection and Comparison:\n",
        "\n",
        "Comparing Models: By comparing the loss values of different models, you can assess their relative performance.\n",
        "\n",
        "Selecting the Best Model: The model with the lowest loss value is often considered the best-performing one.\n",
        "\n",
        "4. Hyperparameter Tuning:\n",
        "\n",
        "Optimizing Hyperparameters: Loss values can guide the tuning of hyperparameters, such as learning rate and regularization strength.\n",
        "\n",
        " By experimenting with different hyperparameter settings, you can aim to minimize the loss.\n",
        "\n",
        " 5. Limitations :\n",
        "\n",
        "* The absolute value of the loss isn't always meaningful by itself; it depends on the scale of the task and loss function.\n",
        "\n",
        "* A low loss doesn't always mean a \"good model\" if:\n",
        "  * The model overfits (good loss on training but poor on validation/test).\n",
        "  * The metric of interest (e.g., accuracy, F1 score) doesn't align with the loss function."
      ],
      "metadata": {
        "id": "6WZEILx9bOI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What are continuous and categorical variables?\n"
      ],
      "metadata": {
        "id": "4FciGf8PcVa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous and Categorical Variables\n",
        "\n",
        "In statistics and data analysis, variables are fundamental building blocks. They can be broadly classified into two types: continuous and categorical.\n",
        "\n",
        "Continuous Variables:\n",
        "\n",
        "* Definition: These variables represent data that can take on any value within a given range and are typically numerical. They are often measurements or quantities.\n",
        "* Key Characteristics:\n",
        "  * Values are part of a continuum with no gaps.\n",
        "  * Can take on fractional or decimal values (e.g., 2.5, 7.89).\n",
        "  * Often associated with an interval or ratio scale.\n",
        "* Examples:\n",
        "  * Height (e.g., 170.5 cm)\n",
        "  * Weight (e.g., 65.2 kg)\n",
        "  * Temperature (e.g., 98.6°F)\n",
        "  * Time (e.g., 2.35 hours)\n",
        "* Usage in Machine Learning:\n",
        "  * Requires scaling (e.g., standardization or normalization) for some algorithms.\n",
        "  * Can be used directly in calculations (e.g., regression models).\n",
        "\n",
        "Categorical Variables :\n",
        "* Definition: These variables represent data that can be grouped into distinct categories or labels and are typically qualitative.\n",
        "* Key Characteristics:\n",
        "  * Values belong to a finite set of categories.\n",
        "  * Cannot take on fractional or numerical values.\n",
        "  * Can be nominal (no inherent order) or ordinal (categories have an order).\n",
        "* Examples:\n",
        "  * Nominal: Color (red, green, blue), Gender (male, female)\n",
        "  * Ordinal: Education level (high school, bachelor’s, master’s)\n",
        "  * Categorical values encoded numerically: Day of the week (e.g., 1 = Monday, 2 = Tuesday)\n",
        "* Usage in Machine Learning:\n",
        "  * Requires encoding for numerical algorithms (e.g., one-hot encoding, label encoding).\n",
        "  * Used for classification tasks or feature grouping.\n"
      ],
      "metadata": {
        "id": "HWnROVAHcm0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How do we handle categorical variables in Machine Learning? What are the common t echniques?"
      ],
      "metadata": {
        "id": "j0zRYQKUeRQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling Categorical Variables in Machine Learning\n",
        "\n",
        "Categorical variables, unlike numerical ones, represent different categories or groups. Since most machine learning algorithms work with numerical data, we need to convert categorical variables into a suitable numerical format. Here are some common techniques:\n",
        "\n",
        "1. One-Hot Encoding\n",
        "* How it works: Creates a new binary feature for each category.\n",
        "* When to use: When there's no inherent order between categories.\n",
        "* Example:\n",
        "  * Color: Red, Green, Blue\n",
        "  * One-hot encoded:\n",
        "    * Red: [1, 0, 0]\n",
        "    * Green: [0, 1, 0]\n",
        "    * Blue: [0, 0, 1]\n",
        "\n",
        "2. Label Encoding\n",
        "* How it works: Assigns a unique integer to each category.\n",
        "* When to use: When categories have a natural order.\n",
        "* Example:\n",
        "  * Education: High School, Bachelor's, Master's, PhD\n",
        "  * Label encoded:\n",
        "    * High School: 1\n",
        "    * Bachelor's: 2\n",
        "    * Master's: 3\n",
        "    * PhD: 4\n",
        "\n",
        "3. Target Encoding\n",
        "* How it works: Replaces each category with the mean target value for that category.\n",
        "* When to use: When you want to capture the relationship between the categorical variable and the target variable.\n",
        "* Example:\n",
        "  * If the target variable is \"Salary,\" and \"Department\" is a categorical variable, each department would be replaced with the average salary of employees in that department.\n",
        "\n",
        "4. Frequency Encoding\n",
        "* How it works: Replaces each category with its frequency in the dataset.\n",
        "* When to use: When the frequency of a category is informative.\n",
        "* Example:\n",
        "  * If \"Brand\" is a categorical variable, a brand that appears more frequently might be more popular or reliable.\n",
        "\n",
        "Choosing the Right Technique:\n",
        "\n",
        "The best technique depends on the specific problem and the machine learning algorithm being used:\n",
        "\n",
        "* One-hot encoding is a versatile technique that works well with many algorithms.\n",
        "* Label encoding is suitable for ordinal categorical variables.\n",
        "* Target encoding can improve model performance, but it can also lead to overfitting.\n",
        "* Frequency encoding is a simple technique that can be effective in some cases.\n",
        "\n"
      ],
      "metadata": {
        "id": "yt4Tc9_5eero"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "TK3jSHJbkOps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Testing a Dataset: A Simplified Explanation\n",
        "\n",
        "Imagine you're teaching a child to recognize different animals. You show them pictures of various animals (dogs, cats, birds, etc.) and tell them what each one is. This is similar to how we train a machine learning model.\n",
        "\n",
        "Training a Dataset:\n",
        "\n",
        "* Data Preparation: The dataset is cleaned and preprocessed to remove errors and inconsistencies.\n",
        "* Model Selection: A suitable machine learning algorithm (like linear regression, decision trees, or neural networks) is chosen.\n",
        "* Model Training: The algorithm is fed the training data, learning patterns and relationships between features (like color, size, shape) and the target variable (the type of animal).\n",
        "* Parameter Tuning: The algorithm's parameters are adjusted to optimize its performance.\n",
        "\n",
        "Testing a Dataset :\n",
        "\n",
        "Once the model is trained, it's time to test its knowledge. We present it with a new set of pictures (the testing data) that it hasn't seen before. The model makes predictions about what each animal is, and we compare its predictions to the actual answers.\n",
        "\n",
        "Why is this important?\n",
        "\n",
        "* Model Evaluation: Testing helps us assess how well the model has learned. A good model should be able to accurately predict on unseen data.\n",
        "* Preventing Overfitting: Overfitting occurs when a model becomes too specialized in the training data and performs poorly on new data. Testing helps identify and mitigate overfitting.\n",
        "* Model Improvement: By analyzing the model's performance on the testing data, we can identify areas for improvement and refine the model."
      ],
      "metadata": {
        "id": "G9qVmDQQk2zj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "e8x1QPyZlUuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing: A Toolkit for Data Preprocessing\n",
        "\n",
        "sklearn.preprocessing is a powerful submodule within the scikit-learn library that provides essential tools for data preprocessing. Data preprocessing is a critical step in machine learning pipelines, as it ensures that data is in a suitable format for model training.\n",
        "\n",
        "Common Preprocessing Techniques Offered by sklearn.preprocessing:\n",
        "\n",
        "1. Scaling :\n",
        "\n",
        "  * StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
        "  * MinMaxScaler: Scales features to a specific range, often between 0 and 1.\n",
        "  * RobustScaler: Scales features using a robust estimator of central tendency and dispersion.\n",
        "\n",
        "\n",
        "2. Normalization:\n",
        "\n",
        "  * Normalizer: Scales input vectors individually to unit norm.\n",
        "3. Encoding Categorical Features:\n",
        "\n",
        "  * OneHotEncoder: Converts categorical features into a one-hot numerical array.\n",
        "  * LabelEncoder: Encodes target labels with values between 0 and n_classes-1.\n",
        "4. Imputation of Missing Values:\n",
        "\n",
        "   Imputer: Imputes missing values using strategies like mean, median, mode, or most frequent value.\n",
        "5. Feature Transformation:\n",
        "\n",
        "  * PolynomialFeatures: Generates polynomial features from input features.\n",
        "  * PowerTransformer: Applies a power transform to make data more Gaussian-like.\n",
        "\n",
        "Why is Preprocessing Important?\n",
        "\n",
        "* Improved Model Performance: Many machine learning algorithms assume that features are on a similar scale. Preprocessing helps ensure that features are comparable, leading to better model performance.\n",
        "* Faster Convergence: Scaling can accelerate the convergence of gradient-based optimization algorithms.\n",
        "* Better Interpretability: Normalization can make feature coefficients more interpretable.\n",
        "* Handling Missing Data: Imputation techniques help handle missing values, preventing data loss and potential bias."
      ],
      "metadata": {
        "id": "Xky3ZYFClatj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is a Test set?"
      ],
      "metadata": {
        "id": "41KSN2yuZp3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A test set is a portion of a dataset that is used to evaluate the performance of a machine learning model. It's a set of data that the model has never seen before, and its purpose is to provide an unbiased assessment of how well the model generalizes to new, unseen data.\n",
        "\n",
        "Key Characteristics of a Test Set:\n",
        "1. Unseen Data: The test set is not used during the training or tuning process. This ensures an unbiased evaluation of the model's performance.\n",
        "2. Evaluation Metric Calculation: Metrics like accuracy, precision, recall, F1 score, or mean squared error are typically calculated using the test set to quantify the model's effectiveness.\n",
        "3. Generalization Performance: The test set provides insights into how the model might perform on real-world data.\n",
        "\n",
        "Here's how the process typically works:\n",
        "\n",
        "1. Data Split: The original dataset is divided into two main parts:\n",
        "\n",
        "  * Training set: Used to train the model.\n",
        "  * Test set: Held back for final evaluation.\n",
        "2. Model Training: The model is trained on the training set, learning patterns and relationships in the data.\n",
        "\n",
        "3. Model Evaluation: Once the model is trained, it's applied to the test set. The model's predictions on the test set are compared to the actual values.\n",
        "\n",
        "4. Performance Metrics: Various metrics, like accuracy, precision, recall, F1-score, and mean squared error, are used to assess the model's performance on the test set.\n",
        "\n",
        "Why is a test set important?\n",
        "\n",
        "* Unbiased Evaluation: It provides an unbiased estimate of the model's performance, as the model hasn't been exposed to this data during training.\n",
        "* Preventing Overfitting: It helps identify if the model is overfitting, which occurs when the model becomes too complex and performs well on the training data but poorly on new data.\n",
        "* Model Selection: It can be used to compare different models and select the best-performing one."
      ],
      "metadata": {
        "id": "0lvwn7mtZ4wR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "3gz44KvwbQyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Splitting Data for Model Fitting (Training and Testing)\n",
        "\n",
        "In Python, you can split your dataset into training and testing sets using the train_test_split function from Scikit-learn. This function randomly divides the dataset while maintaining the proportion of the data.\n",
        "\n",
        "2. Approach to Solving a Machine Learning Problem\n",
        "\n",
        "A structured approach helps systematically solve machine learning problems. Here’s a breakdown:\n",
        "\n",
        "Step 1: Understand the Problem\n",
        "\n",
        "* Define Objectives: What are you trying to predict or classify? Is it a regression, classification, clustering, etc.?\n",
        "* Understand Data Context: What do the features and target variable represent?\n",
        "* Identify Success Metrics: Define metrics like accuracy, precision, recall, F1 score, or mean squared error.\n",
        "\n",
        "Step 2: Data Collection\n",
        "* Collect or access the dataset.\n",
        "* Ensure the dataset is representative of the problem domain.\n",
        "\n",
        "Step 3: Data Exploration and Preprocessing\n",
        "  1. Exploratory Data Analysis (EDA):\n",
        "\n",
        "    * Visualize data to identify patterns, trends, or anomalies.\n",
        "    * Summarize key statistics (mean, median, standard deviation).\n",
        "    * Check for class imbalances, missing values, and outliers.\n",
        "  2. Data Cleaning:\n",
        "\n",
        "    * Handle missing values (impute or remove).\n",
        "    * Remove or correct outliers if necessary.\n",
        "    * Ensure consistent formatting.\n",
        "  3. Feature Engineering:\n",
        "\n",
        "    * Encode categorical variables (e.g., OneHotEncoder or LabelEncoder).\n",
        "    * Scale numerical features (e.g., StandardScaler or MinMaxScaler).\n",
        "    * Create or remove features to improve model performance.\n",
        "  4. Data Splitting:\n",
        "\n",
        "    * Split the dataset into training, validation (if needed), and test sets.\n",
        "\n",
        "Step 4: Model Selection\n",
        "  1. Choose the type of model:\n",
        "    * Linear models, decision trees, support vector machines, neural networks, etc., depending on the problem.\n",
        "  2. Consider baseline models first to set a performance benchmark.\n",
        "\n",
        "Step 5: Model Training\n",
        "* Train the model on the training data.\n",
        "* Use cross-validation (e.g., K-fold cross-validation) for robust evaluation.\n",
        "\n",
        "Step 6: Model Evaluation\n",
        "* Evaluate the model on the validation or test set using metrics suitable for the problem.\n",
        "* Look for overfitting or underfitting.\n",
        "\n",
        "Step 7: Model Tuning\n",
        "1. Hyperparameter Tuning:\n",
        "\n",
        "* Use techniques like grid search or random search with cross-validation.\n",
        "* Optimize parameters like learning rate, depth of trees, etc.\n",
        "2. Feature Selection:\n",
        "\n",
        "* Identify and retain the most important features.\n",
        "* Remove redundant or irrelevant features.\n",
        "\n",
        "Step 8: Deployment\n",
        "* Save the model using libraries like joblib or pickle.\n",
        "* Deploy the model using frameworks like Flask, FastAPI, or cloud services.\n",
        "\n",
        "Step 9: Monitor and Update\n",
        "* Continuously monitor model performance in production.\n",
        "* Update the model with new data to ensure relevance."
      ],
      "metadata": {
        "id": "bKSBInpXcrz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "LWT2rdT6fqUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis (EDA) is a crucial step in the machine learning workflow because it helps you understand your dataset in depth before fitting a model. Skipping EDA can lead to poor model performance, unreliable predictions, or misinterpretations of results. Here's why EDA is important:\n",
        "\n",
        "1. Understand the Data's Structure\n",
        "* Check Data Types: Ensure features have appropriate types (e.g., numeric, categorical).\n",
        "* Feature Relationships: Identify correlations or dependencies between features and the target variable.\n",
        "* Class Distribution: Check for imbalanced datasets in classification problems.\n",
        "\n",
        "  Example: A highly imbalanced dataset (e.g., 95% class A and 5% class B) might require techniques like resampling or using specialized metrics.\n",
        "\n",
        "2. Detect and Handle Missing Data\n",
        "* Missing data can lead to errors or biased models.\n",
        "* EDA helps you decide how to handle it:\n",
        "  * Remove rows/columns with excessive missing values.\n",
        "  * Impute missing values (mean, median, mode, or advanced methods).\n",
        "\n",
        "  Example: A feature with 70% missing values might not add value and could be dropped.\n",
        "\n",
        "3. Identify Outliers\n",
        "* Outliers can skew the training process for some models (e.g., linear regression, k-means).\n",
        "* EDA helps identify and decide whether to:\n",
        "  * Remove them.\n",
        "  * Transform the data (e.g., logarithmic scaling).\n",
        "  * Use models robust to outliers.\n",
        "\n",
        "Example: A single extreme value in a feature like income could lead to poor model performance.\n",
        "\n",
        "4. Understand Feature Distributions\n",
        "* Some models (e.g., logistic regression, neural networks) perform better with normalized or standardized data.\n",
        "* Visualizing feature distributions (e.g., histograms, box plots) helps decide if transformations are needed.\n",
        "  \n",
        "  Example: A highly skewed feature might benefit from a logarithmic or square-root transformation.\n",
        "\n",
        "5. Identify Relationships Between Features\n",
        "* Detect collinearity (high correlation between features) which can affect model performance.\n",
        "* Decide if some features should be dropped or combined.\n",
        "\n",
        "  Example: Two features with 99% correlation might add redundancy, requiring dimensionality reduction (e.g., PCA).\n",
        "\n",
        "6. Choose the Right Model\n",
        "* Certain patterns in the data (e.g., linear vs. non-linear relationships) can guide model selection.\n",
        "* EDA helps you determine whether simpler models (e.g., linear regression) or more complex ones (e.g., neural networks) are appropriate.\n",
        "7. Guide Feature Engineering\n",
        "* EDA helps generate new features or modify existing ones to improve model performance.\n",
        "* For example:\n",
        "  * Combining features to create meaningful interactions.\n",
        "  * Encoding categorical variables based on their relationship with the target.\n",
        "8. Avoid Costly Errors\n",
        "* Data issues like duplicate rows, incorrect entries, or inconsistent formatting can severely impact model performance.\n",
        "* EDA ensures the data is clean and reliable before fitting a model.\n",
        "9. Select Evaluation Metrics\n",
        "* Understanding the data's structure and class distribution helps choose appropriate metrics (e.g., accuracy for balanced data, F1-score for imbalanced data).\n",
        "\n",
        "Tools and Techniques for EDA:\n",
        "1. Visualization: Use libraries like matplotlib, seaborn, and plotly for histograms, scatter plots, box plots, and heatmaps.\n",
        "2. Descriptive Statistics: Use pandas to compute means, medians, standard deviations, and correlations.\n",
        "3. Missing Value Analysis: Use df.isnull().sum() to identify missing values.\n",
        "4. Outlier Detection: Use box plots or interquartile ranges (IQR) to spot outliers."
      ],
      "metadata": {
        "id": "F7h-gLxsfzfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. What is correlation?"
      ],
      "metadata": {
        "id": "-aHxdJsIhtrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation\n",
        "\n",
        " correlation refers to the statistical relationship between two variables. It measures how strongly two variables are related to each other and in what direction.\n",
        "\n",
        " Types of Correlation:\n",
        "\n",
        "1. Positive Correlation:\n",
        "  * As one variable increases, the other also increases.\n",
        "  * Example: Height and Weight\n",
        "2. Negative Correlation:\n",
        "  * As one variable increases, the other decreases.\n",
        "  * Example: Temperature and Ice Cream Sales\n",
        "3. No Correlation:\n",
        "  * No relationship between the two variables.\n",
        "\n",
        "Why is Correlation Important in Machine Learning?\n",
        "1. Feature Selection:\n",
        "\n",
        "  * Helps identify highly correlated features. If two features are strongly correlated, one may be redundant and can be removed.\n",
        "2. Understanding Relationships:\n",
        "\n",
        "  * Helps identify relationships between features and the target variable, guiding model building and feature engineering.\n",
        "3. Detecting Multicollinearity:\n",
        "\n",
        "  * High correlation among independent variables can lead to multicollinearity, which can distort model coefficients in regression.\n",
        "4. Data Exploration:\n",
        "\n",
        "  * Provides insights into patterns and trends in the data.\n",
        "\n",
        "Visualizing Correlation:\n",
        "\n",
        "* Scatter Plots: Visualize the relationship between two variables.\n",
        "* Correlation Matrix: A matrix showing the correlation coefficients between all pairs of variables.\n",
        "\n",
        "Limitations of Correlation\n",
        "1. Does Not Imply Causation:\n",
        "\n",
        "    * A strong correlation does not mean one variable causes the other to change.\n",
        "2. Limited to Linear Relationships:\n",
        "\n",
        "  * Correlation may not detect non-linear relationships effectively.\n",
        "3. Affected by Outliers:\n",
        "\n",
        "  * Extreme values can significantly distort correlation values."
      ],
      "metadata": {
        "id": "gG81crxZiI05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. What does negative correlation mean?"
      ],
      "metadata": {
        "id": "BvASmTkTkHKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative Correlation\n",
        "\n",
        "A negative correlation means that two variables move in opposite directions. As one variable increases, the other decreases.\n",
        "\n",
        "Key Characteristics of Negative Correlation\n",
        "1. Range:\n",
        "\n",
        "  * Correlation values range from 0 to -1 for negative correlation.\n",
        "  * -1: Perfect negative correlation (the variables are inversely proportional in a linear manner).\n",
        "  * Closer to 0: Weak or no negative correlation.\n",
        "2. Direction:\n",
        "\n",
        "  * A negative correlation implies that higher values of one variable are associated with lower values of the other.\n",
        "3. Strength:\n",
        "\n",
        "  * The strength of the relationship increases as the correlation value approaches -1.\n",
        "\n",
        "Limitations\n",
        "* Does Not Imply Causation: A negative correlation between two variables does not mean that one causes the other to decrease.\n",
        "* Affected by Outliers: Extreme values can distort the correlation value.\n",
        "\n",
        "Examples of Negative Correlation\n",
        "1. Real-World Examples:\n",
        "\n",
        "  * Temperature and Sales of Winter Clothes: As temperature rises, winter clothing sales decrease.\n",
        "  * Distance and Signal Strength: As the distance from a Wi-Fi router increases, the signal strength decreases.\n",
        "2. Data Examples:\n",
        "\n",
        "  * Consider these two variables:\n",
        "    * X = [1, 2, 3, 4, 5]\n",
        "    * Y = [10, 8, 6, 4, 2]\n",
        "  * Here, as X increases, Y decreases, showing a negative correlation.  "
      ],
      "metadata": {
        "id": "QkhERlCtklbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "AzzvFlVvmZJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding Correlation Between Variables in Python\n",
        "\n",
        "Correlation is a statistical measure that indicates the strength and direction of the relationship between two variables. In Python, we can use various libraries to calculate correlation, primarily NumPy and Pandas.\n",
        "\n",
        "1. Using NumPy\n",
        "\n",
        "* corrcoef() function: This function calculates the Pearson product-moment correlation coefficient, which measures the linear correlation between two variables."
      ],
      "metadata": {
        "id": "8JDOpn52HQRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "correlation_coefficient = np.corrcoef(x, y)[0, 1]\n",
        "print(\"Correlation coefficient:\", correlation_coefficient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBUkwpwWHcBC",
        "outputId": "4fd1774a-77b2-4c85-8896-36d18ae5eec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation coefficient: 0.7745966692414834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Using Pandas\n",
        "\n",
        "corr() method: This method calculates the correlation matrix, which shows the correlation between all pairs of variables in a DataFrame."
      ],
      "metadata": {
        "id": "5yEldwmZHsBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {'x': [1, 2, 3, 4, 5],\n",
        "        'y': [2, 4, 5, 4, 5],\n",
        "        'z': [3, 5, 7, 9, 11]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0MjawR-HtLs",
        "outputId": "0e118268-2903-4541-db25-40e33f3979e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          x         y         z\n",
            "x  1.000000  0.774597  1.000000\n",
            "y  0.774597  1.000000  0.774597\n",
            "z  1.000000  0.774597  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting Correlation Coefficients\n",
        "\n",
        "* -1 to 0: Negative correlation (as one variable increases, the other decreases)\n",
        "\n",
        "* 0: No correlation\n",
        "* 0 to 1: Positive correlation (as one variable increases, the other also increases)\n",
        "\n",
        "Visualizing Correlation\n",
        "\n",
        "To better understand the relationship between variables, you can visualize them using scatter plots:"
      ],
      "metadata": {
        "id": "5W69IJr_IDsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Scatter Plot of X and Y')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "28DqkYbWImZz",
        "outputId": "139bd6ea-5603-420b-dd19-f2b5a60bb4d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the Right Correlation Coefficient\n",
        "\n",
        "* Pearson correlation coefficient: Assumes a linear relationship between variables and normally distributed data.\n",
        "* Spearman's rank correlation coefficient: Measures monotonic relationships (increasing or decreasing) and is less sensitive to outliers.\n",
        "* Kendall's tau correlation coefficient: Another non-parametric correlation coefficient that measures the strength of association between two ranked variables."
      ],
      "metadata": {
        "id": "slvz_hnaIoqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15 What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "UCSHePK2I0Ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Causation\n",
        "\n",
        "Causation refers to a direct cause-and-effect relationship between two variables. It means that a change in one variable directly results in a change in the other. For example, turning on a light switch causes the light to turn on.\n",
        "\n",
        "* Correlation\n",
        "\n",
        "Correlation, on the other hand, simply indicates a statistical relationship between two variables. It means that two variables tend to change together, but it doesn't necessarily imply that one causes the other.\n",
        "\n",
        "Example: Ice Cream Sales and Drowning Deaths\n",
        "\n",
        "A classic example to illustrate the difference between correlation and causation is the relationship between ice cream sales and drowning deaths. Statistical analysis might show a strong positive correlation between these two variables: when ice cream sales increase, so do drowning deaths.\n",
        "\n",
        "However, this correlation doesn't mean that eating ice cream causes people to drown. The underlying factor driving both is likely the weather: hot summer days lead to increased ice cream consumption and more people swimming, which in turn can lead to more drowning accidents.\n",
        "\n",
        "Key Point:\n",
        "\n",
        "* Correlation does not imply causation.\n",
        "* To establish causation, we need strong evidence, often from controlled experiments or rigorous statistical analysis."
      ],
      "metadata": {
        "id": "EqAm5hTpJ-gB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "1R2ed8HPKJ7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is an Optimizer?\n",
        "\n",
        "In the realm of machine learning, particularly in neural networks, an optimizer is an algorithm that adjusts the model's parameters (weights and biases) to minimize the loss function. This loss function measures the discrepancy between the model's predictions and the actual ground truth values. By iteratively adjusting the parameters, the optimizer aims to find the optimal set of values that minimizes the loss, thereby improving the model's performance.\n",
        "\n",
        "Different Types of Optimizers\n",
        "\n",
        "1. Gradient Descent (GD)\n",
        "\n",
        "* A fundamental optimization algorithm that calculates the gradient of the loss function with respect to each parameter.\n",
        "* It then updates the parameters in the direction of the negative gradient.\n",
        "* Example: Imagine a hiker trying to reach the lowest point in a valley. The hiker takes steps downhill, adjusting their direction based on the steepness of the slope.\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "* A variant of GD that uses a random subset of training data to calculate the gradient at each iteration.\n",
        "* This reduces computational cost and can lead to faster convergence.\n",
        "* Example: Instead of examining the entire landscape, the hiker only looks at a small portion at a time, making quicker adjustments.\n",
        "3. Mini-Batch Gradient Descent\n",
        "\n",
        "* A compromise between GD and SGD, using small batches of training data to update the parameters.\n",
        "* It offers a balance between the stability of GD and the efficiency of SGD.\n",
        "* Example: The hiker now examines a small group of trees at a time, making more informed decisions.\n",
        "4. Momentum\n",
        "\n",
        "* An extension of SGD that adds momentum to the parameter updates.\n",
        "* This helps to accelerate convergence, especially in cases with noisy gradients.\n",
        "* Example: The hiker gains momentum as they descend, allowing them to overcome small obstacles and reach the bottom faster.\n",
        "5. RMSprop (Root Mean Square Propagation)\n",
        "\n",
        "* Adapts the learning rate for each parameter based on the average of recent squared gradients.\n",
        "* This helps to avoid oscillations and accelerate convergence.\n",
        "* Example: The hiker adjusts their step size based on the terrain, taking larger steps on smooth slopes and smaller steps on steeper ones.\n",
        "6. Adam (Adaptive Moment Estimation)\n",
        "\n",
        "* Combines the advantages of Momentum and RMSprop.\n",
        "* It adapts both the learning rate and momentum for each parameter.\n",
        "* Example: The hiker not only adjusts their step size but also their direction based on the terrain, leading to efficient and effective movement.\n",
        "\n",
        "The choice of optimizer depends on various factors, including the size of the dataset, the complexity of the model, and the desired convergence speed.In practice, Adam is often a popular choice due to its robustness and efficiency. However, experimentation is crucial to find the best optimizer for a specific task."
      ],
      "metadata": {
        "id": "I4J2qHZHKm-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17 What is sklearn.linear_model ?\n"
      ],
      "metadata": {
        "id": "ocLz1b0WLtCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.linear_model is a module in the scikit-learn library that provides a variety of linear models for regression and classification tasks. These models are based on the assumption that the relationship between the independent variables (features) and the dependent variable (target) is linear.\n",
        "\n",
        "Key Models in sklearn.linear_model:\n",
        "\n",
        "1. Linear Regression:\n",
        "\n",
        "* Used for predicting a continuous numerical value.\n",
        "* Fits a linear model to minimize the sum of squared errors between predicted and actual values.\n",
        "* Example: Predicting house prices based on features like square footage, number of bedrooms, and location.\n",
        "2. Logistic Regression:\n",
        "\n",
        "* Used for classification tasks, predicting a binary outcome (e.g., 0 or 1).\n",
        "* Fits a logistic function to the linear combination of features to estimate the probability of a positive class.\n",
        "* Example: Predicting whether an email is spam or not spam.\n",
        "3. Ridge Regression:\n",
        "\n",
        "* A regularization technique that adds a penalty term to the loss function to prevent overfitting.\n",
        "* It helps to reduce the impact of correlated features and improve model generalization.\n",
        "* Example: Predicting stock prices with many correlated features.\n",
        "4. Lasso Regression:\n",
        "\n",
        "* Another regularization technique that performs feature selection by setting some coefficients to zero.\n",
        "* This can lead to simpler and more interpretable models.\n",
        "* Example: Identifying the most important factors influencing customer churn.\n",
        "5. Elastic Net Regression:\n",
        "\n",
        "* Combines the features of Ridge and Lasso regression.\n",
        "* It allows for both feature selection and regularization.\n",
        "* Example: Predicting medical costs with a large number of features\n",
        "\n",
        "Common Use Cases:\n",
        "\n",
        "* Predictive Modeling: Forecasting future trends, such as sales or stock prices.\n",
        "* Classification: Categorizing data into different classes, like spam detection or customer segmentation.\n",
        "* Feature Selection: Identifying the most relevant features for a model.\n",
        "* Anomaly Detection: Identifying unusual data points.\n",
        "\n",
        "By understanding these models and their applications, you can effectively use the sklearn.linear_model module to build powerful machine learning models for various tasks."
      ],
      "metadata": {
        "id": "cikZKfVfMQgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18. What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "Wimv-9OdNJB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model.fit() method in scikit-learn is a crucial step in machine learning. It's where the magic happens! Here's a breakdown of its role and the arguments it requires:\n",
        "\n",
        "Function of model.fit()\n",
        "\n",
        "* The model.fit() method is responsible for training a machine learning model.\n",
        "* It takes your training data and uses it to learn the underlying relationships between features (independent variables) and the target variable (what you're trying to predict).\n",
        "* During this process, the model adjusts its internal parameters (weights and biases) to minimize the error between its predictions and the actual values in the training data.\n",
        "\n",
        "Required Arguments\n",
        "\n",
        "1. X (features): This is a 2D array-like object that represents your training data features. It typically has one row per data sample and one column per feature.\n",
        "2. y (target): This is a 1D array-like object that represents the target variable values for each data sample in the training data. The format depends on the specific model:\n",
        "  * Regression: Continuous numerical values.\n",
        "  * Classification: Class labels (e.g., integers or strings representing categories).\n",
        "\n",
        "Optional Arguments\n",
        "\n",
        "While X and y are mandatory, there are several optional arguments you can provide to model.fit() depending on the specific model and your needs. Here are a few common ones:\n",
        "\n",
        "* sample_weight (array-like, shape (n_samples,), default=None): An array of weights that can be used to assign different importance to different samples in the training data.\n",
        "* validation_split (float, int, or None, default=None): This allows you to split the training data into a training set and a validation set for early stopping or hyperparameter tuning.\n",
        "* epochs (int, default=1): This is relevant for some models like neural networks and specifies the number of times the entire training set is passed through the model during training.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "N8N2w4AcOFha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = [[1, 2], [3, 4], [5, 6]]\n",
        "y = [5, 11, 17]\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "QN5q8z5AOpCq",
        "outputId": "0b6cba38-e896-4c9b-8b91-e1fa7672497c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LinearRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19. What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "8VrwyM1gO4K5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.predict() is a fundamental function in machine learning models, particularly in deep learning frameworks like TensorFlow and Keras. It takes input data and generates predictions based on the model's learned parameters.\n",
        "\n",
        "Arguments:\n",
        "\n",
        "The specific arguments required for model.predict() can vary depending on the model's architecture and the framework being used. However, the most common argument is the input data:\n",
        "\n",
        "1. Input Data:\n",
        "* Shape: The input data must be in the same shape as the data used to train the model. This includes the number of samples, features, and any other relevant dimensions.\n",
        "* Type: The data type should be compatible with the model's expected input, typically NumPy arrays or TensorFlow tensors.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "VUvPbTLTvYf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Create and train the model\n",
        "X_train = np.array([[1], [2], [3]])\n",
        "y_train = np.array([2, 4, 6])\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "X_test = np.array([[4], [5]])\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions)  # Output: [8, 10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgOTzmc62Zfn",
        "outputId": "af3bbf46-cb7d-45b5-dcc2-c2fa7bd862b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 8. 10.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Considerations:\n",
        "\n",
        "* Batch Size: Some models may require input data to be batched. The batch size can be specified in the batch_size argument of model.predict().\n",
        "* Model Architecture: The specific input requirements will depend on the model's architecture. For example, a convolutional neural network may expect input images, while a recurrent neural network may expect input sequences.\n",
        "* Preprocessing: The input data may need to be preprocessed before feeding it to the model. This can include normalization, scaling, or other transformations.\n",
        "\n",
        "By understanding the specific requirements of your model and the framework you're using, you can effectively use model.predict() to generate predictions on new data."
      ],
      "metadata": {
        "id": "QRVD9eEQvYN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "W053LtKK24AX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous and Categorical Variables\n",
        "\n",
        "In statistics, variables are used to represent characteristics or attributes of a population or sample. These variables can be broadly classified into two types: continuous and categorical.\n",
        "\n",
        "Continuous Variables\n",
        "\n",
        "* Definition: Continuous variables are those that can take on any value within a given range.\n",
        "* Characteristics:\n",
        "  * Measured on a continuous scale.\n",
        "  * Infinite number of possible values.\n",
        "  * Often represented by real numbers.\n",
        "* Examples:\n",
        "  * Height\n",
        "  * Weight\n",
        "  * Temperature  \n",
        "  * Time\n",
        "  * Income\n",
        "\n",
        "Categorical Variables\n",
        "\n",
        "* Definition: Categorical variables are those that take on a finite number of values, often representing different categories or groups.  \n",
        "* Characteristics:\n",
        "  * Measured on a nominal or ordinal scale.\n",
        "  * Finite number of possible values.\n",
        "  * Often represented by labels or codes.\n",
        "* Examples:\n",
        "  * Gender (male, female)\n",
        "  * Marital status (single, married, divorced)\n",
        "  * Education level (high school, college, graduate)\n",
        "  * Country of origin\n",
        "  * Eye color\n"
      ],
      "metadata": {
        "id": "p-41QKAJ3ow_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21. What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "3QzyjhkE5WB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Scaling: A Crucial Preprocessing Technique\n",
        "\n",
        "Feature scaling is a preprocessing technique in machine learning that involves transforming numerical features of a dataset to a common scale or range. This is essential because many machine learning algorithms, especially those based on distance metrics like K-Nearest Neighbors (KNN) and K-Means Clustering, perform better when features are on a similar scale.  \n",
        "\n",
        "Why Feature Scaling Matters:\n",
        "\n",
        "1. Improved Model Performance:\n",
        "  * Gradient Descent: Algorithms like gradient descent converge faster when features have similar scales. This is because the gradients are more balanced, leading to more efficient optimization.\n",
        "  * Distance-Based Algorithms: Algorithms like KNN and K-Means rely on distance calculations between data points. If features have vastly different scales, the distance metric can be dominated by features with larger scales, leading to inaccurate results.\n",
        "2. Preventing Feature Dominance:\n",
        "  * Features with larger magnitudes can dominate the learning process, overshadowing the impact of features with smaller magnitudes. Feature scaling ensures that all features contribute equally to the model's learning.\n",
        "3. Regularization:\n",
        "  * Some regularization techniques, like L1 and L2 regularization, are sensitive to feature scales. Scaling can help improve the effectiveness of these techniques.\n",
        "\n",
        "Common Feature Scaling Techniques:\n",
        "\n",
        "1. Min-Max Scaling (Normalization):\n",
        "  * Rescales features to a specific range, typically between 0 and 1.  \n",
        "  * Formula: X_scaled = (X - X_min) / (X_max - X_min)\n",
        "2. Standardization (Z-score Normalization):\n",
        "  * Rescales features to have a mean of 0 and a standard deviation of 1.\n",
        "  * Formula: X_scaled = (X - mean(X)) / std(X)\n",
        "\n",
        "When to Use Which Technique:\n",
        "\n",
        "* Min-Max Scaling:\n",
        "  * Suitable for algorithms that are sensitive to feature ranges, like neural networks and support vector machines.  \n",
        "  * Useful when you know the specific range you want to map your features to.\n",
        "* Standardization:\n",
        "  * More robust to outliers and is commonly used with algorithms that assume a normal distribution of features, like linear regression and logistic regression.\n",
        "By applying feature scaling techniques, you can significantly improve the performance and reliability of your machine learning models."
      ],
      "metadata": {
        "id": "G3EdkrDs6b4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22. How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "eCj3v91_8OnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling in Python: A Comprehensive Guide\n",
        "\n",
        "Scaling, or feature scaling, is a crucial preprocessing step in machine learning to ensure that different features contribute equally to the model. It involves transforming numerical features to a common scale.\n",
        "\n",
        "Here are the most common scaling techniques and their Python implementations:\n",
        "\n",
        "1. Min-Max Scaling (Normalization):\n",
        "\n",
        "Rescales features to a specific range, typically between 0 and 1.\n",
        "Useful when you don't have a specific distribution assumption."
      ],
      "metadata": {
        "id": "FGA6M21F9bln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Example data\n",
        "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "\n",
        "# Initialize scaler and fit-transform data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBEbZX-n9ijO",
        "outputId": "f6a785d3-02c8-43db-d03a-fcb420b0d139"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0.  0. ]\n",
            " [0.5 0.5 0.5]\n",
            " [1.  1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Standardization (Z-score Normalization):\n",
        "\n",
        "  * Transforms features to have zero mean and unit variance.\n",
        "  * Assumes a normal distribution.\n",
        "  * Useful for many machine learning algorithms, especially those based on distance metrics."
      ],
      "metadata": {
        "id": "Gbi5lgkF9nUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Example data\n",
        "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "\n",
        "# Initialize scaler and fit-transform data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8QsGxDB-Hpm",
        "outputId": "afcc301a-460c-4cb8-be40-3b81f0433832"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487 -1.22474487]\n",
            " [ 0.          0.          0.        ]\n",
            " [ 1.22474487  1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Robust Scaling:\n",
        "\n",
        "  * Less sensitive to outliers than standard scaling.\n",
        "  * Uses median and interquartile range (IQR) for scaling."
      ],
      "metadata": {
        "id": "nBlPiEcg-HX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Example data\n",
        "data = [[1, 2, 3], [4, 100, 6], [7, 8, 9]]\n",
        "\n",
        "# Initialize scaler and fit-transform data\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QixqKY6P-RdB",
        "outputId": "15cd5727-54c1-4f3b-b0c4-de8fa1f4302d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.         -0.12244898 -1.        ]\n",
            " [ 0.          1.87755102  0.        ]\n",
            " [ 1.          0.          1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use Which Scaling Technique:\n",
        "\n",
        "* Min-Max Scaling:\n",
        "  * When you want to preserve the original distribution shape.\n",
        "  * When you have a limited range of values.\n",
        "* Standardization:\n",
        "  * When features have different scales and units.\n",
        "  * When you assume a normal distribution.\n",
        "* Robust Scaling:\n",
        "  * When your data contains outliers.\n",
        "  * When you want to reduce the influence of outliers.\n",
        "\n",
        "Additional Considerations:\n",
        "\n",
        "* Feature Engineering: Scaling is often combined with other feature engineering techniques like feature selection and dimensionality reduction.\n",
        "* Model Selection: The choice of scaling technique can impact the performance of different machine learning algorithms.\n",
        "* Hyperparameter Tuning: It's essential to tune hyperparameters of your model after scaling to optimize performance.\n",
        "* Data Leakage: Be careful not to introduce data leakage during the scaling process, especially when using test data."
      ],
      "metadata": {
        "id": "rk-dcatu-d-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "zZa3xqHd8W_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a powerful module within the scikit-learn library that provides a range of tools for data preprocessing. It's essential for preparing data before feeding it into machine learning algorithms.\n",
        "\n",
        "Key Functions and Classes:\n",
        "\n",
        "1. Scaling:\n",
        "\n",
        "  * StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
        "  * MinMaxScaler: Scales features to a specific range (usually 0 to 1).\n",
        "  * RobustScaler: Scales features using median and interquartile range, making it robust to outliers.\n",
        "2. Normalization:\n",
        "\n",
        "  * normalize: Normalizes samples individually to unit norm.\n",
        "3. Binarization:\n",
        "\n",
        "  * Binarizer: Thresholds numerical features to binary values (0 or 1).\n",
        "4. Encoding Categorical Features:\n",
        "\n",
        "  * LabelEncoder: Encodes target labels with values between 0 and n_classes-1.\n",
        "  * OneHotEncoder: Converts categorical features into a one-hot numerical array.\n",
        "5. Imputation:\n",
        "\n",
        "  * SimpleImputer: Replaces missing values with a specified strategy (e.g., mean, median, most frequent).\n",
        "6. Polynomial Features:\n",
        "\n",
        "  * PolynomialFeatures: Generates polynomial and interaction features from input data.\n",
        "7. Custom Transformations:\n",
        "\n",
        "  * FunctionTransformer: Applies a custom function to transform features.\n",
        "\n",
        "Why is Preprocessing Important?\n",
        "\n",
        "  * Improved Model Performance: Many machine learning algorithms assume that features are on a similar scale. Preprocessing helps ensure that features contribute equally to the model.\n",
        "  * Faster Convergence: Scaling can speed up the convergence of optimization algorithms.\n",
        "  * Better Interpretability: Some techniques like normalization can make model coefficients more interpretable.\n",
        "\n",
        "Example Usage:"
      ],
      "metadata": {
        "id": "aq9qyliH_YKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a sample dataset\n",
        "data = [[1, -1], [2, 0], [0, 1]]\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the data and transform it\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ByVeVnEAGkg",
        "outputId": "7d2ad25a-dec2-4f33-f4cf-1f4be94ec57d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.         -1.22474487]\n",
            " [ 1.22474487  0.        ]\n",
            " [-1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24. How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "MSHqUo5G8Zkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting Data for Model Fitting in Python\n",
        "\n",
        "To effectively train and evaluate machine learning models, it's crucial to split your dataset into training and testing sets. The training set is used to train the model, while the testing set is used to evaluate its performance on unseen data.\n",
        "\n",
        "Using train_test_split from sklearn.model_selection\n",
        "\n",
        "The most common approach is to use the train_test_split function from the sklearn.model_selection module. Here's a basic example:"
      ],
      "metadata": {
        "id": "3xevmHUhBIW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = [[1], [2], [3], [4], [5]]\n",
        "y = [0, 1, 0, 1, 0]\n",
        "\n",
        "# Assuming X is your feature matrix and y is your target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhd4TXhCBO1e",
        "outputId": "41032b5f-d03e-4749-d62a-bbf78d797dd5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the Parameters:\n",
        "\n",
        "* X: Your feature matrix.\n",
        "* y: Your target variable.\n",
        "* test_size: The proportion of the dataset to include in the test set (default is 0.25).\n",
        "* random_state: A seed value for the random number generator, ensuring reproducibility.\n",
        "\n",
        "Why Splitting is Important:\n",
        "\n",
        "* Model Evaluation: By evaluating the model on unseen data, we can get a reliable estimate of its performance.\n",
        "* Preventing Overfitting: Overfitting occurs when a model becomes too complex and fits the training data too closely, leading to poor performance on new data. Splitting helps mitigate this.\n",
        "* Fair Assessment: A fair split ensures that the model is not biased towards specific data points.\n",
        "\n",
        "Additional Considerations:\n",
        "\n",
        "* Stratified Sampling: If your dataset is imbalanced (unequal class distribution), consider using stratified sampling to ensure that the proportion of classes in the training and testing sets is similar.\n",
        "* Time Series Data: For time series data, special care is needed to maintain the temporal order. You might use time-based splits or cross-validation techniques like time series cross-validation.\n",
        "* Cross-Validation: To get a more robust estimate of model performance, you can use cross-validation techniques like k-fold cross-validation. This involves splitting the data into k folds, training the model on k-1 folds, and evaluating it on the remaining fold. This process is repeated k times, and the average performance is reported."
      ],
      "metadata": {
        "id": "jVUUb9YJBsYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25. Explain data encoding?\n"
      ],
      "metadata": {
        "id": "R5A9Ecom8fG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Encoding: Transforming Categorical Data into Numerical\n",
        "\n",
        "In machine learning, many algorithms require numerical data. However, real-world datasets often contain categorical data, such as gender, color, or country. To make these categorical variables understandable to algorithms, we need to convert them into numerical representations. This process is known as data encoding.\n",
        "\n",
        "Common Data Encoding Techniques:\n",
        "\n",
        "1. Label Encoding:\n",
        "\n",
        "  * Assigns a unique integer to each category.\n",
        "  * Suitable for ordinal categorical variables (where categories have a natural order).  \n",
        "  * Example: Low, Medium, High can be encoded as 0, 1, 2.  \n",
        "2. One-Hot Encoding:\n",
        "\n",
        "  * Creates a new binary feature for each category.  \n",
        "  * Suitable for nominal categorical variables (where categories have no inherent order).  \n",
        "  * Example: Color (Red, Green, Blue) can be encoded as three binary features: Red (0/1), Green (0/1), Blue (0/1).  \n",
        "3. Target Encoding:\n",
        "\n",
        "  * Replaces a categorical feature with the mean target value for that category.\n",
        "  * Useful for features with many unique categories.\n",
        "  * Example: If \"City\" is a categorical feature, replace each city with the average target value for that city.\n",
        "\n",
        "Choosing the Right Encoding Technique:\n",
        "\n",
        "The choice of encoding technique depends on the nature of the categorical variable and the machine learning algorithm being used:\n",
        "\n",
        "* Ordinal Variables: Label encoding is often suitable.\n",
        "* Nominal Variables: One-hot encoding is commonly used.\n",
        "* High-Cardinality Categorical Variables: Target encoding or other advanced techniques like embedding can be effective.\n",
        "\n",
        "Why is Data Encoding Important?\n",
        "\n",
        "* Compatibility with Algorithms: Most machine learning algorithms require numerical input.\n",
        "* Improved Model Performance: Proper encoding can lead to better model performance.\n",
        "* Feature Engineering: Encoding can create new features that capture valuable information."
      ],
      "metadata": {
        "id": "WfQJ1tlgD8bR"
      }
    }
  ]
}